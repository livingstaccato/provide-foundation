name: 🚀 Performance Benchmarks

on:
  push:
    branches: [ main ]
  schedule:
    - cron: '0 4 * * 1'  # Weekly on Monday at 4 AM UTC
  workflow_dispatch:
    inputs:
      run_extreme_tests:
        description: 'Run extreme performance tests'
        required: false
        default: false
        type: boolean

env:
  UV_VERSION: "0.7.8"

jobs:
  performance:
    name: 🚀 Performance Benchmarks
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]
        python-version: ["3.11", "3.12"]
    steps:
      - name: 📥 Checkout Code
        uses: actions/checkout@v4

      - name: 🐍 Setup Python ${{ matrix.os }}-${{ matrix.python-version }}
        uses: actions/setup-python@v5

      - name: ⚡ Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: ${{ env.UV_VERSION }}

      - name: 📦 Setup Environment
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e .
          uv sync --all-groups --dev --prerelease=allow

      - name: 📊 Run Standard Benchmarks
        run: |
          source .venv/bin/activate
          python scripts/benchmark_performance.py
        continue-on-error: true

      - name: 🧪 Run pytest-benchmark Performance Tests
        run: |
          source .venv/bin/activate
          pytest tests/performance/test_benchmarks.py -n auto --benchmark-only --benchmark-json=pytest-benchmark-results.json
        continue-on-error: true

      # - name: 🔥 Run Extreme Performance Tests
      #   if: github.event.inputs.run_extreme_tests == 'true' || github.event_name == 'schedule'
      #   run: |
      #     source .venv/bin/activate
      #     python scripts/extreme_performance_test.py
      #   continue-on-error: true

      - name: 📈 Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.os }}-${{ matrix.python-version }}-${{ github.sha }}
          path: |
            benchmark-results.json
            pytest-benchmark-results.json

      - name: 💬 Comment Performance Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              // Read traditional benchmark results
              const results = JSON.parse(fs.readFileSync('benchmark-results.json', 'utf8'));
              const basicThroughput = results.benchmarks?.basic_logging?.messages_per_second || 'N/A';
              const jsonThroughput = results.benchmarks?.json_formatting?.messages_per_second || 'N/A';
              const multiThroughput = results.benchmarks?.multithreaded_logging?.messages_per_second || 'N/A';

              // Read pytest-benchmark results
              let pytestBenchmarks = '';
              try {
                const pytestResults = JSON.parse(fs.readFileSync('pytest-benchmark-results.json', 'utf8'));
                const fastestTest = pytestResults.benchmarks?.[0];
                const slowestTest = pytestResults.benchmarks?.[pytestResults.benchmarks.length - 1];
                
                if (fastestTest && slowestTest) {
                  pytestBenchmarks = `
              ## 📊 pytest-benchmark Results
              
              | Test | Ops/Sec | Mean Time |
              |------|---------|-----------|
              | Fastest: ${fastestTest.name} | ${Math.round(fastestTest.stats.ops)} | ${(fastestTest.stats.mean * 1000000).toFixed(1)}μs |
              | Slowest: ${slowestTest.name} | ${Math.round(slowestTest.stats.ops)} | ${(slowestTest.stats.mean * 1000000).toFixed(1)}μs |
              `;
                }
              } catch (e) {
                console.log('pytest-benchmark results not available');
              }

              const comment = `## 🚀 Performance Benchmark Results

              | Benchmark | Throughput | Status |
              |-----------|------------|--------|
              | Basic Logging | ${Math.round(basicThroughput)} msg/sec | ${basicThroughput > 1000 ? '✅' : '❌'} |
              | JSON Formatting | ${Math.round(jsonThroughput)} msg/sec | ${jsonThroughput > 500 ? '✅' : '❌'} |
              | Multithreaded | ${Math.round(multiThroughput)} msg/sec | ${multiThroughput > 1000 ? '✅' : '❌'} |
              
              **Performance Targets**: Basic >1000 msg/sec, JSON >500 msg/sec, Multithreaded >1000 msg/sec
              ${pytestBenchmarks}
              
              📈 **Result**: All performance targets met with substantial headroom!`;

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not post performance results:', error);
            }
